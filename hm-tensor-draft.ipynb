{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-26T02:14:29.090941Z","iopub.execute_input":"2022-04-26T02:14:29.091520Z","iopub.status.idle":"2022-04-26T02:14:29.095845Z","shell.execute_reply.started":"2022-04-26T02:14:29.091480Z","shell.execute_reply":"2022-04-26T02:14:29.095045Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow_recommenders","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:14:29.218290Z","iopub.execute_input":"2022-04-26T02:14:29.218704Z","iopub.status.idle":"2022-04-26T02:14:59.151525Z","shell.execute_reply.started":"2022-04-26T02:14:29.218669Z","shell.execute_reply":"2022-04-26T02:14:59.150735Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"### Import necessary libraries\n\nfrom typing import Dict, Text\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorflow_recommenders as tfrs\n\nimport os\nimport pprint\nimport tempfile\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:14:59.153691Z","iopub.execute_input":"2022-04-26T02:14:59.153988Z","iopub.status.idle":"2022-04-26T02:15:03.723644Z","shell.execute_reply.started":"2022-04-26T02:14:59.153930Z","shell.execute_reply":"2022-04-26T02:15:03.722933Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"trans_train = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv')[:100000]\ntrans_train","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:15:03.724849Z","iopub.execute_input":"2022-04-26T02:15:03.725139Z","iopub.status.idle":"2022-04-26T02:16:00.108621Z","shell.execute_reply.started":"2022-04-26T02:15:03.725106Z","shell.execute_reply":"2022-04-26T02:16:00.107973Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# master_df = trans_train[['customer_id','article_id','price']].astype(str)\n# master_df['price'] = master_df['price'].astype(float)\n# masterdf = master_df","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:16:00.110849Z","iopub.execute_input":"2022-04-26T02:16:00.111277Z","iopub.status.idle":"2022-04-26T02:16:00.114715Z","shell.execute_reply.started":"2022-04-26T02:16:00.111237Z","shell.execute_reply":"2022-04-26T02:16:00.114025Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"trans_train['quantity']=1\nmaster_df = trans_train[['customer_id','article_id','quantity']].astype(str)\nmaster_df['quantity'] = master_df['quantity'].astype(float)\nmasterdf = master_df","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:16:00.116208Z","iopub.execute_input":"2022-04-26T02:16:00.116690Z","iopub.status.idle":"2022-04-26T02:16:00.329047Z","shell.execute_reply.started":"2022-04-26T02:16:00.116653Z","shell.execute_reply":"2022-04-26T02:16:00.328336Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"### define interactions data and user data\n\n### interactions \n### here we create a reference table of the user , item, and quantity purchased\ninteractions_dict = masterdf.groupby(['customer_id', 'article_id'])[ 'quantity'].sum().reset_index()\n\n## we tansform the table inta a dictionary , which then we feed into tensor slices\n# this step is crucial as this will be the type of data fed into the embedding layers\ninteractions_dict = {name: np.array(value) for name, value in interactions_dict.items()}\ninteractions = tf.data.Dataset.from_tensor_slices(interactions_dict)\n\n## we do similar step for item, where this is the reference table for items to be recommended\nitems_dict = masterdf[['article_id']].drop_duplicates()\nitems_dict = {name: np.array(value) for name, value in items_dict.items()}\nitems = tf.data.Dataset.from_tensor_slices(items_dict)\n\n## map the features in interactions and items to an identifier that we will use throught the embedding layers\n## do it for all the items in interaction and item table\n## you may often get itemtype error, so that is why here i am casting the quantity type as float to ensure consistency\ninteractions = interactions.map(lambda x: {\n    'customer_id' : x['customer_id'], \n    'article_id' : x['article_id'], \n    'quantity' : float(x['quantity']),\n\n})\n\nitems = items.map(lambda x: x['article_id'])","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:16:00.330335Z","iopub.execute_input":"2022-04-26T02:16:00.330603Z","iopub.status.idle":"2022-04-26T02:16:02.771362Z","shell.execute_reply.started":"2022-04-26T02:16:00.330558Z","shell.execute_reply":"2022-04-26T02:16:02.770268Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# ### define interactions data and user data\n\n# ### interactions \n# ### here we create a reference table of the user , item, and quantity purchased\n# interactions_dict = masterdf.groupby(['customer_id', 'article_id'])[ 'price'].sum().reset_index()\n\n# ## we tansform the table inta a dictionary , which then we feed into tensor slices\n# # this step is crucial as this will be the type of data fed into the embedding layers\n# interactions_dict = {name: np.array(value) for name, value in interactions_dict.items()}\n# interactions = tf.data.Dataset.from_tensor_slices(interactions_dict)\n\n# ## we do similar step for item, where this is the reference table for items to be recommended\n# items_dict = masterdf[['article_id']].drop_duplicates()\n# items_dict = {name: np.array(value) for name, value in items_dict.items()}\n# items = tf.data.Dataset.from_tensor_slices(items_dict)\n\n# ## map the features in interactions and items to an identifier that we will use throught the embedding layers\n# ## do it for all the items in interaction and item table\n# ## you may often get itemtype error, so that is why here i am casting the quantity type as float to ensure consistency\n# interactions = interactions.map(lambda x: {\n#     'customer_id' : x['customer_id'], \n#     'article_id' : x['article_id'], \n#     'price' : float(x['price']),\n\n# })\n\n# items = items.map(lambda x: x['article_id'])","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:16:02.775065Z","iopub.execute_input":"2022-04-26T02:16:02.776988Z","iopub.status.idle":"2022-04-26T02:16:02.784061Z","shell.execute_reply.started":"2022-04-26T02:16:02.776934Z","shell.execute_reply":"2022-04-26T02:16:02.783075Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"unique_item_titles = np.unique(np.concatenate(list(items.batch(1000))))\nunique_user_ids = np.unique(np.concatenate(list(interactions.batch(1_000).map(lambda x: x[\"customer_id\"]))))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:16:02.788292Z","iopub.execute_input":"2022-04-26T02:16:02.791584Z","iopub.status.idle":"2022-04-26T02:16:05.206516Z","shell.execute_reply.started":"2022-04-26T02:16:02.791544Z","shell.execute_reply":"2022-04-26T02:16:05.205784Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"### get unique item and user id's as a lookup table\nunique_item_titles = np.unique(np.concatenate(list(items.batch(1000))))\nunique_user_ids = np.unique(np.concatenate(list(interactions.batch(1_000).map(lambda x: x[\"customer_id\"]))))\n\n# Randomly shuffle data and split between train and test.\ntf.random.set_seed(42)\nshuffled = interactions.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n\ntrain = shuffled.take(60_000)\ntest = shuffled.skip(60_000).take(20_000)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:16:05.209726Z","iopub.execute_input":"2022-04-26T02:16:05.209993Z","iopub.status.idle":"2022-04-26T02:16:07.510813Z","shell.execute_reply.started":"2022-04-26T02:16:05.209958Z","shell.execute_reply":"2022-04-26T02:16:07.510091Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class RetailModel(tfrs.Model):\n\n    def __init__(self, user_model, item_model):\n        super().__init__()\n        \n        ### Candidate model (item)\n        ### This is Keras preprocessing layers to first convert user ids to integers, \n        ### and then convert those to user embeddings via an Embedding layer. \n        ### We use the list of unique user ids we computed earlier as a vocabulary:\n        item_model = tf.keras.Sequential([\n                                        tf.keras.layers.experimental.preprocessing.StringLookup(\n                                        vocabulary=unique_item_titles, mask_token=None),\n                                        tf.keras.layers.Embedding(len(unique_item_titles) + 1, embedding_dimension)\n                                        ])\n        ### we pass the embedding layer into item model\n        self.item_model: tf.keras.Model = item_model\n            \n        ### Query model (users)    \n        user_model = tf.keras.Sequential([\n                                        tf.keras.layers.experimental.preprocessing.StringLookup(\n                                        vocabulary=unique_user_ids, mask_token=None),\n                                        # We add an additional embedding to account for unknown tokens.\n                                        tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n                                        ])\n        self.user_model: tf.keras.Model = user_model\n        \n        ### for retrieval model. we take top-k accuracy as metrics\n        metrics = tfrs.metrics.FactorizedTopK(candidates=items.batch(128).map(item_model))\n        \n        # define the task, which is retrieval                                    )    \n        task = tfrs.tasks.Retrieval(metrics=metrics)\n       \n        self.task: tf.keras.layers.Layer = task\n\n    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n        # We pick out the user features and pass them into the user model.\n        user_embeddings = self.user_model(features[\"customer_id\"])\n        # And pick out the movie features and pass them into the movie model,\n        # getting embeddings back.\n        positive_movie_embeddings = self.item_model(features[\"article_id\"])\n\n        # The task computes the loss and the metrics.\n        return self.task(user_embeddings, positive_movie_embeddings)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:16:07.514179Z","iopub.execute_input":"2022-04-26T02:16:07.514457Z","iopub.status.idle":"2022-04-26T02:16:07.526085Z","shell.execute_reply.started":"2022-04-26T02:16:07.514425Z","shell.execute_reply":"2022-04-26T02:16:07.525230Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"### Fitting and evaluating\n\n### we choose the dimensionality of the query and candicate representation.\nembedding_dimension = 32\n\n## we pass the model, which is the same model we created in the query and candidate tower, into the model\nitem_model = tf.keras.Sequential([\n                                tf.keras.layers.experimental.preprocessing.StringLookup(\n                                vocabulary=unique_item_titles, mask_token=None),\n                                tf.keras.layers.Embedding(len(unique_item_titles) + 1, embedding_dimension)\n                                ])\n\nuser_model = tf.keras.Sequential([\n                                tf.keras.layers.experimental.preprocessing.StringLookup(\n                                vocabulary=unique_user_ids, mask_token=None),\n                                # We add an additional embedding to account for unknown tokens.\n                                tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n                                ])\n\nmodel = RetailModel(user_model, item_model)\n\n# a smaller learning rate may make the model move slower and prone to overfitting, so we stick to 0.1\n# other optimizers, such as SGD and Adam, are listed here https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\nmodel.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n\ncached_train = train.shuffle(100_000).batch(8192).cache()\ncached_test = test.batch(4096).cache()\n\n## fit the model with ten epochs\nmodel_hist = model.fit(cached_train, epochs=2)\n\n#evaluate the model\nmodel.evaluate(cached_test, return_dict=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:16:07.527458Z","iopub.execute_input":"2022-04-26T02:16:07.527718Z","iopub.status.idle":"2022-04-26T02:19:34.222762Z","shell.execute_reply.started":"2022-04-26T02:16:07.527682Z","shell.execute_reply":"2022-04-26T02:19:34.221925Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_item_titles","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:19:34.224082Z","iopub.execute_input":"2022-04-26T02:19:34.224365Z","iopub.status.idle":"2022-04-26T02:19:34.231605Z","shell.execute_reply.started":"2022-04-26T02:19:34.224331Z","shell.execute_reply":"2022-04-26T02:19:34.230835Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"unique_user_ids ","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:19:34.233185Z","iopub.execute_input":"2022-04-26T02:19:34.233888Z","iopub.status.idle":"2022-04-26T02:19:34.242778Z","shell.execute_reply.started":"2022-04-26T02:19:34.233841Z","shell.execute_reply":"2022-04-26T02:19:34.241753Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\nindex.index_from_dataset(items.batch(100).map(lambda items: (items,model.item_model(items))))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:19:34.244403Z","iopub.execute_input":"2022-04-26T02:19:34.244690Z","iopub.status.idle":"2022-04-26T02:19:34.403504Z","shell.execute_reply.started":"2022-04-26T02:19:34.244652Z","shell.execute_reply":"2022-04-26T02:19:34.402839Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"j = '00000dbacae5abe5e23885899a1fa44253a17956c6d1c3d25f88aa139fdfc657'\n_, titles = index(tf.constant([j]),k=12)\nprint(f\"Recommendations for user %s: {titles[0]}\" %(j))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:19:34.404691Z","iopub.execute_input":"2022-04-26T02:19:34.404923Z","iopub.status.idle":"2022-04-26T02:19:34.419801Z","shell.execute_reply.started":"2022-04-26T02:19:34.404890Z","shell.execute_reply":"2022-04-26T02:19:34.419113Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"t = np.array(titles[0])\nl = [el.decode('UTF-8') for el in t]\nprint(l)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:19:34.420962Z","iopub.execute_input":"2022-04-26T02:19:34.421368Z","iopub.status.idle":"2022-04-26T02:19:34.426876Z","shell.execute_reply.started":"2022-04-26T02:19:34.421329Z","shell.execute_reply":"2022-04-26T02:19:34.426091Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"submission_file = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')[:10]\nsubmission_file","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:19:34.428406Z","iopub.execute_input":"2022-04-26T02:19:34.428798Z","iopub.status.idle":"2022-04-26T02:19:38.527533Z","shell.execute_reply.started":"2022-04-26T02:19:34.428760Z","shell.execute_reply":"2022-04-26T02:19:38.526806Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"sub_cust = submission_file[\"customer_id\"]\nsub_cust","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:26:20.210985Z","iopub.execute_input":"2022-04-26T02:26:20.211538Z","iopub.status.idle":"2022-04-26T02:26:20.217551Z","shell.execute_reply.started":"2022-04-26T02:26:20.211499Z","shell.execute_reply":"2022-04-26T02:26:20.216877Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.DataFrame(columns=['Customer_Id', 'Article_Id'])\nsub_df","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:26:26.383328Z","iopub.execute_input":"2022-04-26T02:26:26.383778Z","iopub.status.idle":"2022-04-26T02:26:26.393807Z","shell.execute_reply.started":"2022-04-26T02:26:26.383740Z","shell.execute_reply":"2022-04-26T02:26:26.393130Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"for customer_id in sub_cust:\n    _, titles = index(tf.constant([customer_id]),k=12)\n    t = np.array(titles[0])\n    l = [el.decode('UTF-8') for el in t]\n    l = [str(item).zfill(10) for item in l]\n    l = \" \".join(l)  \n    print(f\"Recommendations for user %s: {l}\" %(customer_id))\n    submission_file[\"prediction\"] = l","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:47:02.003055Z","iopub.execute_input":"2022-04-26T02:47:02.003321Z","iopub.status.idle":"2022-04-26T02:47:02.034074Z","shell.execute_reply.started":"2022-04-26T02:47:02.003292Z","shell.execute_reply":"2022-04-26T02:47:02.033408Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"submission_file","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:47:05.452365Z","iopub.execute_input":"2022-04-26T02:47:05.452903Z","iopub.status.idle":"2022-04-26T02:47:05.461732Z","shell.execute_reply.started":"2022-04-26T02:47:05.452863Z","shell.execute_reply":"2022-04-26T02:47:05.460838Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"j = '00000dbacae5abe5e23885899a1fa44253a17956c6d1c3d25f88aa139fdfc657'\n_, titles = index(tf.constant([j]),k=12)\nprint(f\"Recommendations for user %s: {titles[0]}\" %(j))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T02:27:51.643738Z","iopub.execute_input":"2022-04-26T02:27:51.644026Z","iopub.status.idle":"2022-04-26T02:27:51.653529Z","shell.execute_reply.started":"2022-04-26T02:27:51.643993Z","shell.execute_reply":"2022-04-26T02:27:51.652554Z"},"trusted":true},"execution_count":66,"outputs":[]}]}