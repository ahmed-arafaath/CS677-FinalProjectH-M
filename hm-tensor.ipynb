{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-25T00:02:34.062117Z","iopub.execute_input":"2022-04-25T00:02:34.064903Z","iopub.status.idle":"2022-04-25T00:02:34.071713Z","shell.execute_reply.started":"2022-04-25T00:02:34.064854Z","shell.execute_reply":"2022-04-25T00:02:34.070764Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow_recommenders","metadata":{"execution":{"iopub.status.busy":"2022-04-25T00:09:49.285803Z","iopub.execute_input":"2022-04-25T00:09:49.286048Z","iopub.status.idle":"2022-04-25T00:10:19.589163Z","shell.execute_reply.started":"2022-04-25T00:09:49.286022Z","shell.execute_reply":"2022-04-25T00:10:19.588338Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"### Import necessary libraries\n\nfrom typing import Dict, Text\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorflow_recommenders as tfrs\n\nimport os\nimport pprint\nimport tempfile\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-04-25T00:10:19.591338Z","iopub.execute_input":"2022-04-25T00:10:19.591637Z","iopub.status.idle":"2022-04-25T00:10:20.536469Z","shell.execute_reply.started":"2022-04-25T00:10:19.591600Z","shell.execute_reply":"2022-04-25T00:10:20.535731Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"trans_train = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv')[:100000]\ntrans_train","metadata":{"execution":{"iopub.status.busy":"2022-04-25T00:10:20.537763Z","iopub.execute_input":"2022-04-25T00:10:20.537984Z","iopub.status.idle":"2022-04-25T00:11:16.001982Z","shell.execute_reply.started":"2022-04-25T00:10:20.537953Z","shell.execute_reply":"2022-04-25T00:11:16.001327Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"master_df = trans_train[['customer_id','article_id','price']].astype(str)\nmaster_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T00:11:28.702784Z","iopub.execute_input":"2022-04-25T00:11:28.703027Z","iopub.status.idle":"2022-04-25T00:11:28.914893Z","shell.execute_reply.started":"2022-04-25T00:11:28.703001Z","shell.execute_reply":"2022-04-25T00:11:28.914238Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"master_df['price'] = master_df['price'].astype(float)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T00:12:12.919608Z","iopub.execute_input":"2022-04-25T00:12:12.919858Z","iopub.status.idle":"2022-04-25T00:12:12.944288Z","shell.execute_reply.started":"2022-04-25T00:12:12.919832Z","shell.execute_reply":"2022-04-25T00:12:12.943352Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"masterdf = master_df","metadata":{"execution":{"iopub.status.busy":"2022-04-25T00:12:54.951562Z","iopub.execute_input":"2022-04-25T00:12:54.952344Z","iopub.status.idle":"2022-04-25T00:12:54.957673Z","shell.execute_reply.started":"2022-04-25T00:12:54.952305Z","shell.execute_reply":"2022-04-25T00:12:54.956559Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"### define interactions data and user data\n\n### interactions \n### here we create a reference table of the user , item, and quantity purchased\ninteractions_dict = masterdf.groupby(['customer_id', 'article_id'])[ 'price'].sum().reset_index()\n\n## we tansform the table inta a dictionary , which then we feed into tensor slices\n# this step is crucial as this will be the type of data fed into the embedding layers\ninteractions_dict = {name: np.array(value) for name, value in interactions_dict.items()}\ninteractions = tf.data.Dataset.from_tensor_slices(interactions_dict)\n\n## we do similar step for item, where this is the reference table for items to be recommended\nitems_dict = masterdf[['article_id']].drop_duplicates()\nitems_dict = {name: np.array(value) for name, value in items_dict.items()}\nitems = tf.data.Dataset.from_tensor_slices(items_dict)\n\n## map the features in interactions and items to an identifier that we will use throught the embedding layers\n## do it for all the items in interaction and item table\n## you may often get itemtype error, so that is why here i am casting the quantity type as float to ensure consistency\ninteractions = interactions.map(lambda x: {\n    'customer_id' : x['customer_id'], \n    'article_id' : x['article_id'], \n    'price' : float(x['price']),\n\n})\n\nitems = items.map(lambda x: x['article_id'])","metadata":{"execution":{"iopub.status.busy":"2022-04-25T00:14:04.976165Z","iopub.execute_input":"2022-04-25T00:14:04.976447Z","iopub.status.idle":"2022-04-25T00:14:07.282636Z","shell.execute_reply.started":"2022-04-25T00:14:04.976412Z","shell.execute_reply":"2022-04-25T00:14:07.281869Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"unique_item_titles = np.unique(np.concatenate(list(items.batch(1000))))\nunique_user_ids = np.unique(np.concatenate(list(interactions.batch(1_000).map(lambda x: x[\"customer_id\"]))))","metadata":{"execution":{"iopub.status.busy":"2022-04-25T00:14:37.430384Z","iopub.execute_input":"2022-04-25T00:14:37.430944Z","iopub.status.idle":"2022-04-25T00:14:39.920557Z","shell.execute_reply.started":"2022-04-25T00:14:37.430905Z","shell.execute_reply":"2022-04-25T00:14:39.919656Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"### get unique item and user id's as a lookup table\nunique_item_titles = np.unique(np.concatenate(list(items.batch(1000))))\nunique_user_ids = np.unique(np.concatenate(list(interactions.batch(1_000).map(lambda x: x[\"customer_id\"]))))\n\n# Randomly shuffle data and split between train and test.\ntf.random.set_seed(42)\nshuffled = interactions.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n\ntrain = shuffled.take(60_000)\ntest = shuffled.skip(60_000).take(20_000)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T00:14:59.017512Z","iopub.execute_input":"2022-04-25T00:14:59.019788Z","iopub.status.idle":"2022-04-25T00:15:01.388954Z","shell.execute_reply.started":"2022-04-25T00:14:59.019748Z","shell.execute_reply":"2022-04-25T00:15:01.388228Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class RetailModel(tfrs.Model):\n\n    def __init__(self, user_model, item_model):\n        super().__init__()\n        \n        ### Candidate model (item)\n        ### This is Keras preprocessing layers to first convert user ids to integers, \n        ### and then convert those to user embeddings via an Embedding layer. \n        ### We use the list of unique user ids we computed earlier as a vocabulary:\n        item_model = tf.keras.Sequential([\n                                        tf.keras.layers.experimental.preprocessing.StringLookup(\n                                        vocabulary=unique_item_titles, mask_token=None),\n                                        tf.keras.layers.Embedding(len(unique_item_titles) + 1, embedding_dimension)\n                                        ])\n        ### we pass the embedding layer into item model\n        self.item_model: tf.keras.Model = item_model\n            \n        ### Query model (users)    \n        user_model = tf.keras.Sequential([\n                                        tf.keras.layers.experimental.preprocessing.StringLookup(\n                                        vocabulary=unique_user_ids, mask_token=None),\n                                        # We add an additional embedding to account for unknown tokens.\n                                        tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n                                        ])\n        self.user_model: tf.keras.Model = user_model\n        \n        ### for retrieval model. we take top-k accuracy as metrics\n        metrics = tfrs.metrics.FactorizedTopK(candidates=items.batch(128).map(item_model))\n        \n        # define the task, which is retrieval                                    )    \n        task = tfrs.tasks.Retrieval(metrics=metrics)\n       \n        self.task: tf.keras.layers.Layer = task\n\n    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n        # We pick out the user features and pass them into the user model.\n        user_embeddings = self.user_model(features[\"customer_id\"])\n        # And pick out the movie features and pass them into the movie model,\n        # getting embeddings back.\n        positive_movie_embeddings = self.item_model(features[\"article_id\"])\n\n        # The task computes the loss and the metrics.\n        return self.task(user_embeddings, positive_movie_embeddings)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T00:16:42.793820Z","iopub.execute_input":"2022-04-25T00:16:42.794492Z","iopub.status.idle":"2022-04-25T00:16:42.804908Z","shell.execute_reply.started":"2022-04-25T00:16:42.794459Z","shell.execute_reply":"2022-04-25T00:16:42.804151Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"### Fitting and evaluating\n\n### we choose the dimensionality of the query and candicate representation.\nembedding_dimension = 32\n\n## we pass the model, which is the same model we created in the query and candidate tower, into the model\nitem_model = tf.keras.Sequential([\n                                tf.keras.layers.experimental.preprocessing.StringLookup(\n                                vocabulary=unique_item_titles, mask_token=None),\n                                tf.keras.layers.Embedding(len(unique_item_titles) + 1, embedding_dimension)\n                                ])\n\nuser_model = tf.keras.Sequential([\n                                tf.keras.layers.experimental.preprocessing.StringLookup(\n                                vocabulary=unique_user_ids, mask_token=None),\n                                # We add an additional embedding to account for unknown tokens.\n                                tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n                                ])\n\nmodel = RetailModel(user_model, item_model)\n\n# a smaller learning rate may make the model move slower and prone to overfitting, so we stick to 0.1\n# other optimizers, such as SGD and Adam, are listed here https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\nmodel.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n\ncached_train = train.shuffle(100_000).batch(8192).cache()\ncached_test = test.batch(4096).cache()\n\n## fit the model with ten epochs\nmodel_hist = model.fit(cached_train, epochs=2)\n\n#evaluate the model\nmodel.evaluate(cached_test, return_dict=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T00:17:08.156350Z","iopub.execute_input":"2022-04-25T00:17:08.157204Z","iopub.status.idle":"2022-04-25T00:19:48.095934Z","shell.execute_reply.started":"2022-04-25T00:17:08.157169Z","shell.execute_reply":"2022-04-25T00:19:48.095230Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model.evaluate(cached_test, return_dict=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T00:19:48.097438Z","iopub.execute_input":"2022-04-25T00:19:48.098339Z","iopub.status.idle":"2022-04-25T00:20:06.661725Z","shell.execute_reply.started":"2022-04-25T00:19:48.098301Z","shell.execute_reply":"2022-04-25T00:20:06.661062Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}