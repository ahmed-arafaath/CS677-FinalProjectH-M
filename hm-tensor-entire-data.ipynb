{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-26T15:46:24.120561Z","iopub.execute_input":"2022-04-26T15:46:24.121208Z","iopub.status.idle":"2022-04-26T15:46:24.143728Z","shell.execute_reply.started":"2022-04-26T15:46:24.121111Z","shell.execute_reply":"2022-04-26T15:46:24.143014Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow_recommenders","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:46:24.145341Z","iopub.execute_input":"2022-04-26T15:46:24.145863Z","iopub.status.idle":"2022-04-26T15:46:53.377501Z","shell.execute_reply.started":"2022-04-26T15:46:24.145826Z","shell.execute_reply":"2022-04-26T15:46:53.376703Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"### Import necessary libraries\n\nfrom typing import Dict, Text\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorflow_recommenders as tfrs\n\nimport os\nimport pprint\nimport tempfile\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:46:53.380056Z","iopub.execute_input":"2022-04-26T15:46:53.380498Z","iopub.status.idle":"2022-04-26T15:46:57.923037Z","shell.execute_reply.started":"2022-04-26T15:46:53.380459Z","shell.execute_reply":"2022-04-26T15:46:57.922282Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"trans_train = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv')\ntrans_train","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:46:57.924349Z","iopub.execute_input":"2022-04-26T15:46:57.924674Z","iopub.status.idle":"2022-04-26T15:47:57.065319Z","shell.execute_reply.started":"2022-04-26T15:46:57.924637Z","shell.execute_reply":"2022-04-26T15:47:57.064621Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# master_df = trans_train[['customer_id','article_id','price']].astype(str)\n# master_df['price'] = master_df['price'].astype(float)\n# masterdf = master_df","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:47:57.067424Z","iopub.execute_input":"2022-04-26T15:47:57.067841Z","iopub.status.idle":"2022-04-26T15:47:57.073279Z","shell.execute_reply.started":"2022-04-26T15:47:57.067804Z","shell.execute_reply":"2022-04-26T15:47:57.072468Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"trans_train['quantity']=1\nmasterdf = trans_train[['customer_id','article_id','quantity']].astype(str)\nmasterdf['quantity'] = masterdf['quantity'].astype(float)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:47:57.074945Z","iopub.execute_input":"2022-04-26T15:47:57.075209Z","iopub.status.idle":"2022-04-26T15:47:57.286934Z","shell.execute_reply.started":"2022-04-26T15:47:57.075173Z","shell.execute_reply":"2022-04-26T15:47:57.286185Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"del trans_train","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:47:57.288331Z","iopub.execute_input":"2022-04-26T15:47:57.288601Z","iopub.status.idle":"2022-04-26T15:47:57.292601Z","shell.execute_reply.started":"2022-04-26T15:47:57.288554Z","shell.execute_reply":"2022-04-26T15:47:57.291818Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"### define interactions data and user data\n\n### interactions \n### here we create a reference table of the user , item, and quantity purchased\ninteractions_dict = masterdf.groupby(['customer_id', 'article_id'])[ 'quantity'].sum().reset_index()\n\n## we tansform the table inta a dictionary , which then we feed into tensor slices\n# this step is crucial as this will be the type of data fed into the embedding layers\ninteractions_dict = {name: np.array(value) for name, value in interactions_dict.items()}\ninteractions = tf.data.Dataset.from_tensor_slices(interactions_dict)\n\n## we do similar step for item, where this is the reference table for items to be recommended\nitems_dict = masterdf[['article_id']].drop_duplicates()\nitems_dict = {name: np.array(value) for name, value in items_dict.items()}\nitems = tf.data.Dataset.from_tensor_slices(items_dict)\n\n## map the features in interactions and items to an identifier that we will use throught the embedding layers\n## do it for all the items in interaction and item table\n## you may often get itemtype error, so that is why here i am casting the quantity type as float to ensure consistency\ninteractions = interactions.map(lambda x: {\n    'customer_id' : x['customer_id'], \n    'article_id' : x['article_id'], \n    'quantity' : float(x['quantity']),\n\n})\n\nitems = items.map(lambda x: x['article_id'])","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:47:57.294038Z","iopub.execute_input":"2022-04-26T15:47:57.294302Z","iopub.status.idle":"2022-04-26T15:47:59.583942Z","shell.execute_reply.started":"2022-04-26T15:47:57.294267Z","shell.execute_reply":"2022-04-26T15:47:59.583217Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# ### define interactions data and user data\n\n# ### interactions \n# ### here we create a reference table of the user , item, and quantity purchased\n# interactions_dict = masterdf.groupby(['customer_id', 'article_id'])[ 'price'].sum().reset_index()\n\n# ## we tansform the table inta a dictionary , which then we feed into tensor slices\n# # this step is crucial as this will be the type of data fed into the embedding layers\n# interactions_dict = {name: np.array(value) for name, value in interactions_dict.items()}\n# interactions = tf.data.Dataset.from_tensor_slices(interactions_dict)\n\n# ## we do similar step for item, where this is the reference table for items to be recommended\n# items_dict = masterdf[['article_id']].drop_duplicates()\n# items_dict = {name: np.array(value) for name, value in items_dict.items()}\n# items = tf.data.Dataset.from_tensor_slices(items_dict)\n\n# ## map the features in interactions and items to an identifier that we will use throught the embedding layers\n# ## do it for all the items in interaction and item table\n# ## you may often get itemtype error, so that is why here i am casting the quantity type as float to ensure consistency\n# interactions = interactions.map(lambda x: {\n#     'customer_id' : x['customer_id'], \n#     'article_id' : x['article_id'], \n#     'price' : float(x['price']),\n\n# })\n\n# items = items.map(lambda x: x['article_id'])","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:47:59.585054Z","iopub.execute_input":"2022-04-26T15:47:59.585833Z","iopub.status.idle":"2022-04-26T15:47:59.591157Z","shell.execute_reply.started":"2022-04-26T15:47:59.585788Z","shell.execute_reply":"2022-04-26T15:47:59.590359Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"unique_item_titles = np.unique(np.concatenate(list(items.batch(1000))))\nunique_user_ids = np.unique(np.concatenate(list(interactions.batch(1_000).map(lambda x: x[\"customer_id\"]))))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:47:59.592673Z","iopub.execute_input":"2022-04-26T15:47:59.592925Z","iopub.status.idle":"2022-04-26T15:48:01.995110Z","shell.execute_reply.started":"2022-04-26T15:47:59.592890Z","shell.execute_reply":"2022-04-26T15:48:01.994302Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"### get unique item and user id's as a lookup table\nunique_item_titles = np.unique(np.concatenate(list(items.batch(1000))))\nunique_user_ids = np.unique(np.concatenate(list(interactions.batch(1_000).map(lambda x: x[\"customer_id\"]))))\n\n# Randomly shuffle data and split between train and test.\ntf.random.set_seed(42)\nshuffled = interactions.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n\ntrain = shuffled.take(60_000)\ntest = shuffled.skip(60_000).take(20_000)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:48:01.998080Z","iopub.execute_input":"2022-04-26T15:48:01.998472Z","iopub.status.idle":"2022-04-26T15:48:04.538098Z","shell.execute_reply.started":"2022-04-26T15:48:01.998430Z","shell.execute_reply":"2022-04-26T15:48:04.537143Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class RetailModel(tfrs.Model):\n\n    def __init__(self, user_model, item_model):\n        super().__init__()\n        \n        ### Candidate model (item)\n        ### This is Keras preprocessing layers to first convert user ids to integers, \n        ### and then convert those to user embeddings via an Embedding layer. \n        ### We use the list of unique user ids we computed earlier as a vocabulary:\n        item_model = tf.keras.Sequential([\n                                        tf.keras.layers.experimental.preprocessing.StringLookup(\n                                        vocabulary=unique_item_titles, mask_token=None),\n                                        tf.keras.layers.Embedding(len(unique_item_titles) + 1, embedding_dimension)\n                                        ])\n        ### we pass the embedding layer into item model\n        self.item_model: tf.keras.Model = item_model\n            \n        ### Query model (users)    \n        user_model = tf.keras.Sequential([\n                                        tf.keras.layers.experimental.preprocessing.StringLookup(\n                                        vocabulary=unique_user_ids, mask_token=None),\n                                        # We add an additional embedding to account for unknown tokens.\n                                        tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n                                        ])\n        self.user_model: tf.keras.Model = user_model\n        \n        ### for retrieval model. we take top-k accuracy as metrics\n        metrics = tfrs.metrics.FactorizedTopK(candidates=items.batch(128).map(item_model))\n        \n        # define the task, which is retrieval                                    )    \n        task = tfrs.tasks.Retrieval(metrics=metrics)\n       \n        self.task: tf.keras.layers.Layer = task\n\n    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n        # We pick out the user features and pass them into the user model.\n        user_embeddings = self.user_model(features[\"customer_id\"])\n        # And pick out the movie features and pass them into the movie model,\n        # getting embeddings back.\n        positive_movie_embeddings = self.item_model(features[\"article_id\"])\n\n        # The task computes the loss and the metrics.\n        return self.task(user_embeddings, positive_movie_embeddings)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:48:04.539362Z","iopub.execute_input":"2022-04-26T15:48:04.539631Z","iopub.status.idle":"2022-04-26T15:48:04.556388Z","shell.execute_reply.started":"2022-04-26T15:48:04.539599Z","shell.execute_reply":"2022-04-26T15:48:04.555688Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"### Fitting and evaluating\n\n### we choose the dimensionality of the query and candicate representation.\nembedding_dimension = 32\n\n## we pass the model, which is the same model we created in the query and candidate tower, into the model\nitem_model = tf.keras.Sequential([\n                                tf.keras.layers.experimental.preprocessing.StringLookup(\n                                vocabulary=unique_item_titles, mask_token=None),\n                                tf.keras.layers.Embedding(len(unique_item_titles) + 1, embedding_dimension)\n                                ])\n\nuser_model = tf.keras.Sequential([\n                                tf.keras.layers.experimental.preprocessing.StringLookup(\n                                vocabulary=unique_user_ids, mask_token=None),\n                                # We add an additional embedding to account for unknown tokens.\n                                tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n                                ])\n\nmodel = RetailModel(user_model, item_model)\n\n# a smaller learning rate may make the model move slower and prone to overfitting, so we stick to 0.1\n# other optimizers, such as SGD and Adam, are listed here https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\nmodel.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n\ncached_train = train.shuffle(100_000).batch(8192).cache()\ncached_test = test.batch(4096).cache()\n\n## fit the model with ten epochs\nmodel_hist = model.fit(cached_train, epochs=2)\n\n#evaluate the model\nmodel.evaluate(cached_test, return_dict=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:48:04.557548Z","iopub.execute_input":"2022-04-26T15:48:04.558475Z","iopub.status.idle":"2022-04-26T15:50:27.413809Z","shell.execute_reply.started":"2022-04-26T15:48:04.558411Z","shell.execute_reply":"2022-04-26T15:50:27.412740Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_item_titles","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:50:27.415287Z","iopub.execute_input":"2022-04-26T15:50:27.415578Z","iopub.status.idle":"2022-04-26T15:50:27.425546Z","shell.execute_reply.started":"2022-04-26T15:50:27.415528Z","shell.execute_reply":"2022-04-26T15:50:27.424623Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"unique_user_ids ","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:50:27.426770Z","iopub.execute_input":"2022-04-26T15:50:27.426968Z","iopub.status.idle":"2022-04-26T15:50:27.437014Z","shell.execute_reply.started":"2022-04-26T15:50:27.426935Z","shell.execute_reply":"2022-04-26T15:50:27.436226Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\nindex.index_from_dataset(items.batch(100).map(lambda items: (items,model.item_model(items))))","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:50:27.438555Z","iopub.execute_input":"2022-04-26T15:50:27.439635Z","iopub.status.idle":"2022-04-26T15:50:27.822059Z","shell.execute_reply.started":"2022-04-26T15:50:27.439597Z","shell.execute_reply":"2022-04-26T15:50:27.821346Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# j = '00000dbacae5abe5e23885899a1fa44253a17956c6d1c3d25f88aa139fdfc657'\n# _, titles = index(tf.constant([j]),k=12)\n# print(f\"Recommendations for user %s: {titles[0]}\" %(j))\n# t = np.array(titles[0])\n# l = [el.decode('UTF-8') for el in t]\n# print(l)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:50:27.823191Z","iopub.execute_input":"2022-04-26T15:50:27.823622Z","iopub.status.idle":"2022-04-26T15:50:27.827967Z","shell.execute_reply.started":"2022-04-26T15:50:27.823582Z","shell.execute_reply":"2022-04-26T15:50:27.827180Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"submission_file = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\nsubmission_file","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:50:27.829440Z","iopub.execute_input":"2022-04-26T15:50:27.829817Z","iopub.status.idle":"2022-04-26T15:50:32.183378Z","shell.execute_reply.started":"2022-04-26T15:50:27.829779Z","shell.execute_reply":"2022-04-26T15:50:32.182564Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"lst = []\n#cust_id = []\nfor customer_id in submission_file[\"customer_id\"]:\n    _, titles = index(tf.constant([customer_id]),k=12)\n    t = np.array(titles[0])\n    l = [el.decode('UTF-8') for el in t]\n    l = [str(item).zfill(10) for item in l]\n    l = \" \".join(l)  \n    #print(f\"Recommendations for user %s: {l}\" %(customer_id))\n    lst.append(l)\n    #cust_id.append(customer_id)\n    #submission_file[\"customer_id\"] = l","metadata":{"execution":{"iopub.status.busy":"2022-04-26T15:50:32.184657Z","iopub.execute_input":"2022-04-26T15:50:32.185285Z","iopub.status.idle":"2022-04-26T16:27:09.196889Z","shell.execute_reply.started":"2022-04-26T15:50:32.185239Z","shell.execute_reply":"2022-04-26T16:27:09.196019Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"submission_file['article_id'] = lst","metadata":{"execution":{"iopub.status.busy":"2022-04-26T16:27:09.198251Z","iopub.execute_input":"2022-04-26T16:27:09.198501Z","iopub.status.idle":"2022-04-26T16:27:09.346451Z","shell.execute_reply.started":"2022-04-26T16:27:09.198469Z","shell.execute_reply":"2022-04-26T16:27:09.345724Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"submission_file","metadata":{"execution":{"iopub.status.busy":"2022-04-26T16:27:09.347875Z","iopub.execute_input":"2022-04-26T16:27:09.348107Z","iopub.status.idle":"2022-04-26T16:27:09.360186Z","shell.execute_reply.started":"2022-04-26T16:27:09.348075Z","shell.execute_reply":"2022-04-26T16:27:09.359531Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"submission_file.drop(\"prediction\" , axis = 1 , inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T16:29:08.298830Z","iopub.execute_input":"2022-04-26T16:29:08.299185Z","iopub.status.idle":"2022-04-26T16:29:08.607046Z","shell.execute_reply.started":"2022-04-26T16:29:08.299144Z","shell.execute_reply":"2022-04-26T16:29:08.606243Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"submission_file = submission_file.set_index('customer_id')\nsubmission_file = submission_file.rename(columns={\"article_id\": \"prediction\"})","metadata":{"execution":{"iopub.status.busy":"2022-04-26T16:34:41.873264Z","iopub.execute_input":"2022-04-26T16:34:41.873931Z","iopub.status.idle":"2022-04-26T16:34:41.937381Z","shell.execute_reply.started":"2022-04-26T16:34:41.873895Z","shell.execute_reply":"2022-04-26T16:34:41.936623Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"submission_file.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T16:34:44.500964Z","iopub.execute_input":"2022-04-26T16:34:44.501706Z","iopub.status.idle":"2022-04-26T16:34:44.509797Z","shell.execute_reply.started":"2022-04-26T16:34:44.501667Z","shell.execute_reply":"2022-04-26T16:34:44.508963Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"submission_file.to_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-26T16:34:57.218050Z","iopub.execute_input":"2022-04-26T16:34:57.218300Z","iopub.status.idle":"2022-04-26T16:35:07.955312Z","shell.execute_reply.started":"2022-04-26T16:34:57.218270Z","shell.execute_reply":"2022-04-26T16:35:07.954414Z"},"trusted":true},"execution_count":29,"outputs":[]}]}