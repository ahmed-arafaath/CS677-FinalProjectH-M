{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6040ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Is this notebook running on Colab or Kaggle?\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from typing import Dict, Text\n",
    "from tensorflow import keras\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import tensorflow_recommenders as tfrs\n",
    "try:\n",
    "    if not tf.config.list_physical_devices('GPU'):\n",
    "        assert tf.__version__ >= \"2.0\"\n",
    "        print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
    "        if IS_COLAB:\n",
    "            print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "        if IS_KAGGLE:\n",
    "            print(\"Go to Settings > Accelerator and select GPU.\")\n",
    "except:\n",
    "    if not tf.test.is_gpu_available():\n",
    "        assert tf.__version__ >= \"2.0\"\n",
    "        print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
    "        if IS_COLAB:\n",
    "            print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "        if IS_KAGGLE:\n",
    "            print(\"Go to Settings > Accelerator and select GPU.\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "try:\n",
    "    if not tf.config.list_physical_devices('GPU'):\n",
    "        tf.random.set_seed(42)\n",
    "    else:\n",
    "        tf.random.set_random_seed(42)\n",
    "except:\n",
    "    if not tf.test.is_gpu_available():\n",
    "        tf.random.set_seed(42)\n",
    "    else:\n",
    "        tf.random.set_random_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af88b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_f(item):\n",
    "    item=str(item)\n",
    "    tem=len(item)\n",
    "    if(len(item)<10):\n",
    "        item=item.zfill(10)\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1f820ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_train = pd.read_csv('h-and-m-personalized-fashion-recommendations/transactions_train.csv',dtype={'customer_id': str,'article_id':str})\n",
    "trans_train['quantity']=1\n",
    "articles= pd.read_csv('h-and-m-personalized-fashion-recommendations/articles.csv',dtype={'article_id': str,'product_code':str})\n",
    "master_df = trans_train[['customer_id','article_id']].astype(str)\n",
    "master_df['dt']=pd.to_datetime(trans_train['t_dat'],format=\"%Y-%m-%d\")\n",
    "master_df['article_id']=master_df['article_id'].apply(zero_f)\n",
    "master_df['dt']=master_df.dt.values.astype(np.int64)\n",
    "master_df['quantity'] = trans_train['quantity'].astype(float)\n",
    "masterdf = master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6632fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_dict = masterdf.groupby(['customer_id', 'article_id', 'dt'])[ 'quantity'].sum().reset_index()\n",
    "interactions_dict = {name: np.array(value) for name, value in interactions_dict.items()}\n",
    "interactions = tf.data.Dataset.from_tensor_slices(interactions_dict)\n",
    "items_dict = articles[['article_id']].drop_duplicates()\n",
    "items_dict = {name: np.array(value) for name, value in items_dict.items()}\n",
    "items = tf.data.Dataset.from_tensor_slices(items_dict)\n",
    "interactions = interactions.map(lambda x: {\n",
    "                                            'customer_id' : x['customer_id'], \n",
    "                                            'article_id' : x['article_id'],\n",
    "                                            'quantity' : float(x['quantity']),\n",
    "                                            \"dt\": x[\"dt\"] })\n",
    "items = items.map(lambda x: x['article_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f11f01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = np.concatenate(list(interactions.map(lambda x: x[\"dt\"]).batch(10000)))\n",
    "\n",
    "max_timestamp = timestamps.max()\n",
    "min_timestamp = timestamps.min()\n",
    "\n",
    "timestamp_buckets = np.linspace(\n",
    "    min_timestamp, max_timestamp, num=1000,\n",
    ")\n",
    "### get unique item and user id's as a lookup table\n",
    "unique_items = np.unique(np.concatenate(list(items.batch(1_000))))\n",
    "unique_user_ids = np.unique(np.concatenate(list(interactions.batch(1_000_000).map(lambda x: x[\"customer_id\"]))))\n",
    "\n",
    "# Randomly shuffle data and split between train and test.\n",
    "tf.random.set_seed(42)\n",
    "shuffled = interactions.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(60_000)\n",
    "test = shuffled.skip(60_000).take(20_000)\n",
    "cached_train = train.shuffle(100_000).batch(2048)\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87194fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, use_timestamps):\n",
    "        super().__init__()\n",
    "\n",
    "        self._use_timestamps = use_timestamps\n",
    "\n",
    "        ## embed user id from unique_user_ids\n",
    "        self.user_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=unique_user_ids, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),\n",
    "        ])\n",
    "\n",
    "        ## embed timestamp\n",
    "        if use_timestamps:\n",
    "            self.timestamp_embedding = tf.keras.Sequential([\n",
    "              tf.keras.layers.Discretization(timestamp_buckets.tolist()),\n",
    "              tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32),\n",
    "            ])\n",
    "            self.normalized_timestamp = tf.keras.layers.Normalization(axis=None)\n",
    "\n",
    "            self.normalized_timestamp.adapt(timestamps)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if not self._use_timestamps:\n",
    "              return self.user_embedding(inputs[\"customer_id\"])\n",
    "\n",
    "        ## all features here\n",
    "        return tf.concat([\n",
    "            self.user_embedding(inputs[\"customer_id\"]),\n",
    "            self.timestamp_embedding(inputs[\"dt\"]),\n",
    "            tf.reshape(self.normalized_timestamp(inputs[\"dt\"]), (-1, 1)),\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "408aa3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        max_tokens = 10_000\n",
    "\n",
    "        ## embed title from unique_item_titles\n",
    "        self.title_embedding = tf.keras.Sequential([\n",
    "                      tf.keras.layers.StringLookup(\n",
    "                          vocabulary=unique_items, mask_token=None),\n",
    "                      tf.keras.layers.Embedding(len(unique_items) + 1, 32)])\n",
    "\n",
    "        ## processing text features: item title vectorizer (see self.title_vectorizer)\n",
    "        self.title_vectorizer = tf.keras.layers.TextVectorization(\n",
    "            max_tokens=max_tokens)\n",
    "\n",
    "        ## we apply title vectorizer to items\n",
    "        self.title_text_embedding = tf.keras.Sequential([\n",
    "                              self.title_vectorizer,\n",
    "                              tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "                              tf.keras.layers.GlobalAveragePooling1D(),])\n",
    "\n",
    "        self.title_vectorizer.adapt(items)\n",
    "\n",
    "    def call(self, titles):\n",
    "        return tf.concat([\n",
    "            self.title_embedding(titles),\n",
    "            self.title_text_embedding(titles),], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06ec28a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CandidateModel(tfrs.models.Model):\n",
    "    def __init__(self, use_timestamps):\n",
    "        super().__init__()\n",
    "\n",
    "        ## query model is user model\n",
    "        self.query_model = tf.keras.Sequential([\n",
    "                          UserModel(use_timestamps),\n",
    "                          tf.keras.layers.Dense(64,activation=\"relu\"),\n",
    "                          tf.keras.layers.Dense(32)])\n",
    "        \n",
    "        ## candidate model is the item model\n",
    "        self.candidate_model = tf.keras.Sequential([\n",
    "                              ItemModel(),\n",
    "                              tf.keras.layers.Dense(64,activation=\"relu\"),\n",
    "                              tf.keras.layers.Dense(32)])\n",
    "        \n",
    "        ## retrieval task, choose metrics\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "                    metrics=tfrs.metrics.FactorizedTopK(\n",
    "                        candidates=items.batch(128).map(self.candidate_model),),)\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        # We only pass the user id and timestamp features into the query model. This\n",
    "        # is to ensure that the training inputs would have the same keys as the\n",
    "        # query inputs. Otherwise the discrepancy in input structure would cause an\n",
    "        # error when loading the query model after saving it.\n",
    "        \n",
    "        query_embeddings = self.query_model({ \"customer_id\": features[\"customer_id\"],\n",
    "                                               \"dt\": features[\"dt\"],})\n",
    "        \n",
    "        item_embeddings = self.candidate_model(features[\"article_id\"])\n",
    "\n",
    "        return self.task(query_embeddings, item_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dcfe736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'customer_id': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=string>, 'dt': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=int64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'customer_id': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=string>, 'dt': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=int64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "30/30 [==============================] - 111s 4s/step - factorized_top_k/top_1_categorical_accuracy: 0.0373 - factorized_top_k/top_5_categorical_accuracy: 0.0509 - factorized_top_k/top_10_categorical_accuracy: 0.0569 - factorized_top_k/top_50_categorical_accuracy: 0.0746 - factorized_top_k/top_100_categorical_accuracy: 0.0861 - loss: 15176.2591 - regularization_loss: 0.0000e+00 - total_loss: 15176.2591\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 112s 4s/step - factorized_top_k/top_1_categorical_accuracy: 0.0157 - factorized_top_k/top_5_categorical_accuracy: 0.0247 - factorized_top_k/top_10_categorical_accuracy: 0.0312 - factorized_top_k/top_50_categorical_accuracy: 0.0532 - factorized_top_k/top_100_categorical_accuracy: 0.0683 - loss: 13862.2675 - regularization_loss: 0.0000e+00 - total_loss: 13862.2675\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 109s 4s/step - factorized_top_k/top_1_categorical_accuracy: 0.0241 - factorized_top_k/top_5_categorical_accuracy: 0.0394 - factorized_top_k/top_10_categorical_accuracy: 0.0479 - factorized_top_k/top_50_categorical_accuracy: 0.0774 - factorized_top_k/top_100_categorical_accuracy: 0.0967 - loss: 13195.8334 - regularization_loss: 0.0000e+00 - total_loss: 13195.8334\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 113s 4s/step - factorized_top_k/top_1_categorical_accuracy: 0.0343 - factorized_top_k/top_5_categorical_accuracy: 0.0515 - factorized_top_k/top_10_categorical_accuracy: 0.0611 - factorized_top_k/top_50_categorical_accuracy: 0.0979 - factorized_top_k/top_100_categorical_accuracy: 0.1244 - loss: 12475.0232 - regularization_loss: 0.0000e+00 - total_loss: 12475.0232\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 111s 4s/step - factorized_top_k/top_1_categorical_accuracy: 0.0367 - factorized_top_k/top_5_categorical_accuracy: 0.0607 - factorized_top_k/top_10_categorical_accuracy: 0.0754 - factorized_top_k/top_50_categorical_accuracy: 0.1315 - factorized_top_k/top_100_categorical_accuracy: 0.1729 - loss: 11690.9005 - regularization_loss: 0.0000e+00 - total_loss: 11690.9005\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 120s 4s/step - factorized_top_k/top_1_categorical_accuracy: 0.0406 - factorized_top_k/top_5_categorical_accuracy: 0.0722 - factorized_top_k/top_10_categorical_accuracy: 0.0921 - factorized_top_k/top_50_categorical_accuracy: 0.1705 - factorized_top_k/top_100_categorical_accuracy: 0.2256 - loss: 10928.8418 - regularization_loss: 0.0000e+00 - total_loss: 10928.8418\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 135s 4s/step - factorized_top_k/top_1_categorical_accuracy: 0.0560 - factorized_top_k/top_5_categorical_accuracy: 0.1015 - factorized_top_k/top_10_categorical_accuracy: 0.1305 - factorized_top_k/top_50_categorical_accuracy: 0.2349 - factorized_top_k/top_100_categorical_accuracy: 0.2984 - loss: 10186.1004 - regularization_loss: 0.0000e+00 - total_loss: 10186.1004\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 124s 4s/step - factorized_top_k/top_1_categorical_accuracy: 0.0624 - factorized_top_k/top_5_categorical_accuracy: 0.1218 - factorized_top_k/top_10_categorical_accuracy: 0.1587 - factorized_top_k/top_50_categorical_accuracy: 0.2755 - factorized_top_k/top_100_categorical_accuracy: 0.3446 - loss: 9511.6528 - regularization_loss: 0.0000e+00 - total_loss: 9511.6528\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 116s 4s/step - factorized_top_k/top_1_categorical_accuracy: 0.0860 - factorized_top_k/top_5_categorical_accuracy: 0.1621 - factorized_top_k/top_10_categorical_accuracy: 0.2044 - factorized_top_k/top_50_categorical_accuracy: 0.3290 - factorized_top_k/top_100_categorical_accuracy: 0.3979 - loss: 8825.1841 - regularization_loss: 0.0000e+00 - total_loss: 8825.1841\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 118s 4s/step - factorized_top_k/top_1_categorical_accuracy: 0.0860 - factorized_top_k/top_5_categorical_accuracy: 0.1801 - factorized_top_k/top_10_categorical_accuracy: 0.2345 - factorized_top_k/top_50_categorical_accuracy: 0.3783 - factorized_top_k/top_100_categorical_accuracy: 0.4529 - loss: 8238.2273 - regularization_loss: 0.0000e+00 - total_loss: 8238.2273\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'customer_id': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=string>, 'dt': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=int64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "5/5 [==============================] - 42s 8s/step - factorized_top_k/top_1_categorical_accuracy: 3.5000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0023 - factorized_top_k/top_10_categorical_accuracy: 0.0041 - factorized_top_k/top_50_categorical_accuracy: 0.0125 - factorized_top_k/top_100_categorical_accuracy: 0.0219 - loss: 44857.5716 - regularization_loss: 0.0000e+00 - total_loss: 44857.5716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.0003499999875202775,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.0023499999660998583,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.004149999935179949,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.012500000186264515,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.021900000050663948,\n",
       " 'loss': 40616.44921875,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 40616.44921875}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CandidateModel(use_timestamps=True)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "model.fit(cached_train, epochs=10)\n",
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4b6f841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x25eaabeed00>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = tfrs.layers.factorized_top_k.BruteForce(model.query_model)\n",
    "index.index_from_dataset(items.batch(100).map(lambda items: (items,model.candidate_model(items))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8357bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'customer_id': <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
      "array([b'000231cc9af9e58ab4edc66fbd61da921b144ba85bc1c00d0ae2309531e4c210'],\n",
      "      dtype=object)>, 'dt': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([879024327])>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "Top recommendations: [b'0575533011' b'0671433004' b'0399136009']\n"
     ]
    }
   ],
   "source": [
    "_, titles = index({\"customer_id\": np.array([\"000231cc9af9e58ab4edc66fbd61da921b144ba85bc1c00d0ae2309531e4c210\"]),\n",
    "    \"dt\": np.array([879024327])}, k=3)\n",
    "print(f\"Top recommendations: {titles[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12424e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "start = datetime.datetime(2020, 9, 23)\n",
    "dt_array = np.array([start + datetime.timedelta(days=i) for i in range(7)])\n",
    "from datetime import datetime\n",
    "squares = np.array([datetime.timestamp(xi) for xi in dt_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "295df55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.6008336e+09, 1.6009200e+09, 1.6010064e+09, 1.6010928e+09,\n",
       "       1.6011792e+09, 1.6012656e+09, 1.6013520e+09])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a4fbeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'customer_id': <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
      "array([b'000231cc9af9e58ab4edc66fbd61da921b144ba85bc1c00d0ae2309531e4c210'],\n",
      "      dtype=object)>, 'dt': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.6008335e+09], dtype=float32)>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "Top recommendations: [b'0575533011' b'0671433004' b'0399136009' b'0594834002' b'0667499009'\n",
      " b'0644873003' b'0667491011' b'0717593001' b'0683356005' b'0696628002'\n",
      " b'0598806001' b'0639091008']\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'customer_id': <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
      "array([b'000231cc9af9e58ab4edc66fbd61da921b144ba85bc1c00d0ae2309531e4c210'],\n",
      "      dtype=object)>, 'dt': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.60092e+09], dtype=float32)>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "Top recommendations: [b'0575533011' b'0671433004' b'0399136009' b'0594834002' b'0667499009'\n",
      " b'0644873003' b'0667491011' b'0717593001' b'0683356005' b'0696628002'\n",
      " b'0598806001' b'0639091008']\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'customer_id': <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
      "array([b'000231cc9af9e58ab4edc66fbd61da921b144ba85bc1c00d0ae2309531e4c210'],\n",
      "      dtype=object)>, 'dt': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.6010063e+09], dtype=float32)>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "Top recommendations: [b'0575533011' b'0671433004' b'0399136009' b'0594834002' b'0667499009'\n",
      " b'0644873003' b'0667491011' b'0717593001' b'0683356005' b'0696628002'\n",
      " b'0598806001' b'0639091008']\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'customer_id': <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
      "array([b'000231cc9af9e58ab4edc66fbd61da921b144ba85bc1c00d0ae2309531e4c210'],\n",
      "      dtype=object)>, 'dt': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.6010929e+09], dtype=float32)>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "Top recommendations: [b'0575533011' b'0671433004' b'0399136009' b'0594834002' b'0667499009'\n",
      " b'0644873003' b'0667491011' b'0717593001' b'0683356005' b'0696628002'\n",
      " b'0598806001' b'0639091008']\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'customer_id': <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
      "array([b'000231cc9af9e58ab4edc66fbd61da921b144ba85bc1c00d0ae2309531e4c210'],\n",
      "      dtype=object)>, 'dt': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.6011791e+09], dtype=float32)>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "Top recommendations: [b'0575533011' b'0671433004' b'0399136009' b'0594834002' b'0667499009'\n",
      " b'0644873003' b'0667491011' b'0717593001' b'0683356005' b'0696628002'\n",
      " b'0598806001' b'0639091008']\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'customer_id': <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
      "array([b'000231cc9af9e58ab4edc66fbd61da921b144ba85bc1c00d0ae2309531e4c210'],\n",
      "      dtype=object)>, 'dt': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.6012657e+09], dtype=float32)>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "Top recommendations: [b'0575533011' b'0671433004' b'0399136009' b'0594834002' b'0667499009'\n",
      " b'0644873003' b'0667491011' b'0717593001' b'0683356005' b'0696628002'\n",
      " b'0598806001' b'0639091008']\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'customer_id': <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
      "array([b'000231cc9af9e58ab4edc66fbd61da921b144ba85bc1c00d0ae2309531e4c210'],\n",
      "      dtype=object)>, 'dt': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.601352e+09], dtype=float32)>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "Top recommendations: [b'0575533011' b'0671433004' b'0399136009' b'0594834002' b'0667499009'\n",
      " b'0644873003' b'0667491011' b'0717593001' b'0683356005' b'0696628002'\n",
      " b'0598806001' b'0639091008']\n"
     ]
    }
   ],
   "source": [
    "for item in squares:\n",
    "    _, titles = index({\"customer_id\": np.array([\"000231cc9af9e58ab4edc66fbd61da921b144ba85bc1c00d0ae2309531e4c210\"]),\n",
    "    \"dt\": np.array([item])}, k=12)\n",
    "    print(f\"Top recommendations: {titles[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b5449",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
