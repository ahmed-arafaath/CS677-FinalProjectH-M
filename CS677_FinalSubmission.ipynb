{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>CS677 - Deep Learning Final Project</h1> \n<hr>\n\nProject team members - \n\n<i>Sajin Shajee</i><br>\n<i>Sanjeet Navinbhai Gajjar</i><br>\n<i>Ahamed Arafaath Muthalif Mubarak Ali</i><br>\n<i>Rohit Subramanian</i>   ","metadata":{}},{"cell_type":"code","source":"!pip install -q tensorflow-recommenders\n!pip install -q scann","metadata":{"execution":{"iopub.status.busy":"2022-05-08T23:11:16.156933Z","iopub.execute_input":"2022-05-08T23:11:16.157319Z","iopub.status.idle":"2022-05-08T23:11:41.902590Z","shell.execute_reply.started":"2022-05-08T23:11:16.157286Z","shell.execute_reply":"2022-05-08T23:11:41.901431Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"<h2><u>Load essential packages</u></h2>","metadata":{}},{"cell_type":"code","source":"# Python ≥3.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n\n# Is this notebook running on Colab or Kaggle?\nIS_COLAB = \"google.colab\" in sys.modules\nIS_KAGGLE = \"kaggle_secrets\" in sys.modules\n\n# Scikit-Learn ≥0.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\n# TensorFlow ≥2.0 is required\nimport tensorflow as tf\nfrom typing import Dict, Text\nfrom tensorflow import keras\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport tensorflow_recommenders as tfrs\ntry:\n    if not tf.config.list_physical_devices('GPU'):\n        assert tf.__version__ >= \"2.0\"\n        print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n        if IS_COLAB:\n            print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n        if IS_KAGGLE:\n            print(\"Go to Settings > Accelerator and select GPU.\")\nexcept:\n    if not tf.test.is_gpu_available():\n        assert tf.__version__ >= \"2.0\"\n        print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n        if IS_COLAB:\n            print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n        if IS_KAGGLE:\n            print(\"Go to Settings > Accelerator and select GPU.\")\n\n# Common imports\nimport numpy as np\nimport pandas as pd\nimport sklearn.preprocessing\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport os\nimport datetime as dt\nfrom pathlib import Path\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\ntry:\n    if not tf.config.list_physical_devices('GPU'):\n        tf.random.set_seed(42)\n    else:\n        tf.random.set_seed(42)\nexcept:\n    if not tf.test.is_gpu_available():\n        tf.random.set_seed(42)\n    else:\n        tf.random.set_seed(42)\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T23:07:07.469833Z","iopub.execute_input":"2022-05-08T23:07:07.470458Z","iopub.status.idle":"2022-05-08T23:07:07.557989Z","shell.execute_reply.started":"2022-05-08T23:07:07.470406Z","shell.execute_reply":"2022-05-08T23:07:07.556914Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"<h4><u>Function zero_f</u></h4>\nInput: Rows of the dataframe.</br>\nOutput: Converted features with standardized length of 10 characters.<hr>\nDescription: Converts customer id to string then adds to the right until the length of id is 10 characters long.","metadata":{}},{"cell_type":"code","source":"# Function to convert customer id to string then adds to the right until the length of id is 10 characters long.\ndef zero_f(item):\n    item=str(item)\n    tem=len(item)\n    if(len(item)<10):\n        item=item.zfill(10)\n    return item","metadata":{"execution":{"iopub.status.busy":"2022-05-08T23:07:07.561432Z","iopub.execute_input":"2022-05-08T23:07:07.561951Z","iopub.status.idle":"2022-05-08T23:07:08.096681Z","shell.execute_reply.started":"2022-05-08T23:07:07.561902Z","shell.execute_reply":"2022-05-08T23:07:08.095433Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"<h2><u>Data Loading and Preprocessing with tensorflow</u></h2>","metadata":{}},{"cell_type":"code","source":"# Load transaction data \ntrans_train = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv',\n                          dtype={'customer_id': str,'article_id':str})\ntrans_train['quantity']=1\ntrans_train = trans_train[trans_train['t_dat'] >'2020-08-01']\n\n# Load article and customers data\narticles= pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/articles.csv',\n                      dtype={'article_id': str,'product_code':str})\ncustomers = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/customers.csv',\n                        dtype={'customer_id':str})\n\n#Feature transformation\nmaster_df = trans_train[['customer_id','article_id','t_dat']].astype(str)\nmaster_df['article_id']=master_df['article_id'].apply(zero_f)\nmaster_df['quantity'] = trans_train['quantity'].astype(float)\nmasterdf = master_df\n\nlen(np.unique(trans_train['customer_id']))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T23:07:08.102546Z","iopub.execute_input":"2022-05-08T23:07:08.102932Z","iopub.status.idle":"2022-05-08T23:08:06.354741Z","shell.execute_reply.started":"2022-05-08T23:07:08.102859Z","shell.execute_reply":"2022-05-08T23:08:06.353268Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"masterdf.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T23:08:06.356272Z","iopub.execute_input":"2022-05-08T23:08:06.356935Z","iopub.status.idle":"2022-05-08T23:08:06.380577Z","shell.execute_reply.started":"2022-05-08T23:08:06.356883Z","shell.execute_reply":"2022-05-08T23:08:06.379594Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"articles.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T23:08:06.382353Z","iopub.execute_input":"2022-05-08T23:08:06.383024Z","iopub.status.idle":"2022-05-08T23:08:06.418646Z","shell.execute_reply.started":"2022-05-08T23:08:06.382980Z","shell.execute_reply":"2022-05-08T23:08:06.417715Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"customers.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T23:08:46.185733Z","iopub.execute_input":"2022-05-08T23:08:46.186044Z","iopub.status.idle":"2022-05-08T23:08:46.205761Z","shell.execute_reply.started":"2022-05-08T23:08:46.186013Z","shell.execute_reply":"2022-05-08T23:08:46.204654Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Feature Transformation\ninteractions = masterdf\ninteractions['t_dat']=pd.to_datetime(interactions['t_dat'])\n\n# Select time frame for model training\ntrain = interactions[interactions['t_dat']<='2020-09-15']\nvalid = interactions[(interactions['t_dat'] <='2019-09-17')&(interactions['t_dat'] >'2019-09-15')]\ntest = interactions[interactions['t_dat'] >'2019-09-17']\nprint(len(np.unique(train['customer_id'])))\n\n#Batcch processing and loading with tensorflow\ntrain_ds = tf.data.Dataset.from_tensor_slices(dict(train[['customer_id',\n                                                          'article_id']])).shuffle(100_000).batch(256).cache()\nvalid_ds = tf.data.Dataset.from_tensor_slices(dict(valid[['customer_id',\n                                                          'article_id']])).batch(256).cache()\ntest_ds = tf.data.Dataset.from_tensor_slices(dict(test[['customer_id',\n                                                        'article_id']])).batch(256).cache()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T23:08:47.383206Z","iopub.execute_input":"2022-05-08T23:08:47.383882Z","iopub.status.idle":"2022-05-08T23:08:51.546974Z","shell.execute_reply.started":"2022-05-08T23:08:47.383833Z","shell.execute_reply":"2022-05-08T23:08:51.546014Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#Feature Engineering\nitems_dict = articles[['article_id']].drop_duplicates()\ncustomer_dict=customers[['customer_id']].drop_duplicates()\n\nitems_dict = {name: np.array(value) for name, value in items_dict.items()}\ncustomer_dict={name:np.array(value) for name,value in customer_dict.items()}\n\ncustomers=tf.data.Dataset.from_tensor_slices(customer_dict)\nitems = tf.data.Dataset.from_tensor_slices(items_dict)\n\nitems = items.map(lambda x: x['article_id'])\ncustomers=customers.map(lambda x: x['customer_id'])","metadata":{"execution":{"iopub.status.busy":"2022-05-08T23:08:51.583345Z","iopub.execute_input":"2022-05-08T23:08:51.584008Z","iopub.status.idle":"2022-05-08T23:08:52.742260Z","shell.execute_reply.started":"2022-05-08T23:08:51.583976Z","shell.execute_reply":"2022-05-08T23:08:52.741152Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"### get unique item and user id's as a lookup table\nunique_items = np.unique(np.concatenate(list(items.batch(1_000))))\nunique_user_ids = np.unique(np.concatenate(list(customers.batch(1_000))))\n\n# Randomly shuffle data and split between train and test.\ntf.random.set_seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T23:08:52.747951Z","iopub.execute_input":"2022-05-08T23:08:52.748453Z","iopub.status.idle":"2022-05-08T23:08:57.334755Z","shell.execute_reply.started":"2022-05-08T23:08:52.748406Z","shell.execute_reply":"2022-05-08T23:08:57.333769Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"<h3><u>Function my_l1_regularizer and my_positive_weights</u></h3>\nInput: Trainable weights <br>\nOutput: Updated weights <hr>\nDescription: Converts input weights to positive value and regularized value","metadata":{}},{"cell_type":"code","source":"# Hyperparamters for model training\ndef my_l1_regularizer(weights):\n    return tf.reduce_sum(tf.abs(0.01 * weights))\ndef my_positive_weights(weights): # return value is just tf.nn.relu(weights)\n    return tf.where(weights < 0., tf.zeros_like(weights), weights)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T23:08:57.337416Z","iopub.execute_input":"2022-05-08T23:08:57.337660Z","iopub.status.idle":"2022-05-08T23:08:57.343594Z","shell.execute_reply.started":"2022-05-08T23:08:57.337632Z","shell.execute_reply":"2022-05-08T23:08:57.342653Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"<h2><u>Model Training</u></h2>\n\nOriginal Model","metadata":{}},{"cell_type":"code","source":"class CandidateModel(tfrs.Model):\n\n    def __init__(self, user_model, item_model):\n        super().__init__()\n        \n        self.item_model: tf.keras.Model = item_model\n        self.user_model: tf.keras.Model = user_model\n        \n        ### for retrieval model. we take top-k accuracy as metrics\n        metrics = tfrs.metrics.FactorizedTopK(candidates=items.batch(256).map(item_model))\n        \n        # define the task, which is retrieval                                    )    \n        task = tfrs.tasks.Retrieval(metrics=metrics)\n       \n        self.task: tf.keras.layers.Layer = task\n\n    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n        # We pick out the user features and pass them into the user model.\n        user_embeddings = self.user_model(features[\"customer_id\"])\n        # And pick out the movie features and pass them into the movie model,\n        # getting embeddings back.\n        article_embeddings = self.item_model(features[\"article_id\"])\n\n        # The task computes the loss and the metrics.\n        return self.task(user_embeddings, article_embeddings)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T23:08:57.345409Z","iopub.execute_input":"2022-05-08T23:08:57.346140Z","iopub.status.idle":"2022-05-08T23:08:57.358254Z","shell.execute_reply.started":"2022-05-08T23:08:57.346084Z","shell.execute_reply":"2022-05-08T23:08:57.357405Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"### Fitting and evaluating\n\n### we choose the dimensionality of the query and candicate representation.\nembedding_dimension = 64\n\n## we pass the model, which is the same model we created in the query and candidate tower, into the model\nitem_model = tf.keras.Sequential([\n                                tf.keras.layers.StringLookup(\n                                vocabulary=unique_items, mask_token=None),\n                                tf.keras.layers.Embedding(len(unique_items) + 1, embedding_dimension)\n                                ])\n\nuser_model = tf.keras.Sequential([\n                                tf.keras.layers.StringLookup(\n                                vocabulary=unique_user_ids, mask_token=None),\n                                # We add an additional embedding to account for unknown tokens.\n                                tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n                                ])\n\nmodel_1 = CandidateModel(user_model, item_model)\nmodel_1.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\nmodel_1.fit(train_ds,epochs=5,batch_size=128)\nmodel_1.evaluate(test_ds, return_dict=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T23:14:26.432799Z","iopub.execute_input":"2022-05-08T23:14:26.433128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate submission file for kaggle submission\nsubmission_file = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv',\n                              dtype={'customer_id': str})\nsub_cust = submission_file[\"customer_id\"]\nsub_df = pd.DataFrame(columns=['Customer_Id', 'Article_Id'])\nsubmission_file[\"prediction\"] = submission_file['customer_id'].progress_apply(run_f)\nsubmission_file","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_file.to_csv('submission_1.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><u>Model Improvement</u></h3>","metadata":{}},{"cell_type":"code","source":"### Fitting and evaluating\n\n### we choose the dimensionality of the query and candicate representation.\nembedding_dimension = 64\n\n## we pass the model, which is the same model we created in the query and candidate tower, into the model\nitem_model = tf.keras.Sequential([\n                                tf.keras.layers.StringLookup(\n                                vocabulary=unique_items, mask_token=None),\n                                tf.keras.layers.Embedding(len(unique_items) + 1, embedding_dimension),\n                                ])\n\nuser_model = tf.keras.Sequential([\n                                tf.keras.layers.StringLookup(\n                                vocabulary=unique_user_ids, mask_token=None),\n                                # We add an additional embedding to account for unknown tokens.\n                                tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension),\n                                tf.keras.layers.Flatten(),\n                                tf.keras.layers.Dense((embedding_dimension),activation=\"relu\",\n                                                      kernel_initializer='he_normal',use_bias=False),\n                                tf.keras.layers.BatchNormalization(),\n                                tf.keras.layers.Dropout(0.2),\n                                tf.keras.layers.Dense((embedding_dimension),activation=\"relu\",\n                                                      kernel_initializer='he_normal',use_bias=False),\n                                tf.keras.layers.BatchNormalization(),\n                                tf.keras.layers.Dropout(0.2),\n                                tf.keras.layers.Dense((embedding_dimension),\n                                                      kernel_regularizer=my_l1_regularizer,\n                                                      kernel_constraint=my_positive_weights)\n                                ])\n\nmodel_2 = CandidateModel(user_model, item_model)\nmodel_2.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\nmodel_2.fit(train_ds,validation_data=valid_ds,epochs=5,batch_size=128)\nmodel_2.evaluate(test_ds, return_dict=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Vocabolary bucket of unique items\nunique_items","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Bucket of unique customer ids\nunique_user_ids ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><u>Model evaluation</u></h2>","metadata":{}},{"cell_type":"code","source":"index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\nindex.index_from_dataset(items.batch(100).map(lambda items: (items,model.item_model(items))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicted recommendations\nj = '000231cc9af9e58ab4edc66fbd61da921b144ba85bc1c00d0ae2309531e4c210'\n_, titles = index(tf.constant([j]),k=12)\nprint(f\"Recommendations for user %s: {titles[0]}\" %(j))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><u>Function decoder and run_f</u></h3>\nInput: feature from submission file <br>\nOutput: decoded and vectorized variable <hr>","metadata":{}},{"cell_type":"code","source":"def decoder(e):\n    return e.decode('UTF-8')\ndef run_f(item):\n    _, titles = index(tf.constant([item]),k=12)\n    t = np.array(titles[0])\n    vfunc = np.vectorize(decoder)\n    l = vfunc(t)\n    l = \" \".join(l)\n    return l","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This indicates the process that was used to convert values for each customer into a string of values. These values are added into the csv file. TQDM provides a progress bar that indicates the progress of the transformations indicated with the apply function.","metadata":{}},{"cell_type":"code","source":"sub_cust = submission_file[\"customer_id\"]\nsub_df = pd.DataFrame(columns=['Customer_Id', 'Article_Id'])\nsubmission_file[\"prediction\"] = submission_file['customer_id'].progress_apply(run_f)\nsubmission_file.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_file.to_csv('submission_2.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}