{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CS677 - Deep Learning Final Project</h1> \n",
    "<hr>\n",
    "\n",
    "Project team members - \n",
    "\n",
    "<i>Sajin Shajee</i><br>\n",
    "<i>Sanjeet Navinbhai Gajjar</i><br>\n",
    "<i>Ahamed Arafaath Muthalif Mubarak Ali</i><br>\n",
    "<i>Rohit Subramanian</i>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T23:11:16.157319Z",
     "iopub.status.busy": "2022-05-08T23:11:16.156933Z",
     "iopub.status.idle": "2022-05-08T23:11:41.902590Z",
     "shell.execute_reply": "2022-05-08T23:11:41.901431Z",
     "shell.execute_reply.started": "2022-05-08T23:11:16.157286Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-recommenders\n",
    "!pip install -q scann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><u>Load essential packages</u></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T23:07:07.470458Z",
     "iopub.status.busy": "2022-05-08T23:07:07.469833Z",
     "iopub.status.idle": "2022-05-08T23:07:07.557989Z",
     "shell.execute_reply": "2022-05-08T23:07:07.556914Z",
     "shell.execute_reply.started": "2022-05-08T23:07:07.470406Z"
    }
   },
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Is this notebook running on Colab or Kaggle?\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from typing import Dict, Text\n",
    "from tensorflow import keras\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import tensorflow_recommenders as tfrs\n",
    "try:\n",
    "    if not tf.config.list_physical_devices('GPU'):\n",
    "        assert tf.__version__ >= \"2.0\"\n",
    "        print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
    "        if IS_COLAB:\n",
    "            print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "        if IS_KAGGLE:\n",
    "            print(\"Go to Settings > Accelerator and select GPU.\")\n",
    "except:\n",
    "    if not tf.test.is_gpu_available():\n",
    "        assert tf.__version__ >= \"2.0\"\n",
    "        print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
    "        if IS_COLAB:\n",
    "            print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "        if IS_KAGGLE:\n",
    "            print(\"Go to Settings > Accelerator and select GPU.\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "try:\n",
    "    if not tf.config.list_physical_devices('GPU'):\n",
    "        tf.random.set_seed(42)\n",
    "    else:\n",
    "        tf.random.set_seed(42)\n",
    "except:\n",
    "    if not tf.test.is_gpu_available():\n",
    "        tf.random.set_seed(42)\n",
    "    else:\n",
    "        tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><u>Function zero_f</u></h4>\n",
    "Input: Rows of the dataframe.</br>\n",
    "Output: Converted features with standardized length of 10 characters.<hr>\n",
    "Description: Converts customer id to string then adds to the right until the length of id is 10 characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T23:07:07.561951Z",
     "iopub.status.busy": "2022-05-08T23:07:07.561432Z",
     "iopub.status.idle": "2022-05-08T23:07:08.096681Z",
     "shell.execute_reply": "2022-05-08T23:07:08.095433Z",
     "shell.execute_reply.started": "2022-05-08T23:07:07.561902Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to convert customer id to string then adds to the right until the length of id is 10 characters long.\n",
    "def zero_f(item):\n",
    "    item=str(item)\n",
    "    tem=len(item)\n",
    "    if(len(item)<10):\n",
    "        item=item.zfill(10)\n",
    "    return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><u>Data Loading and Preprocessing with tensorflow</u></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T23:07:08.102932Z",
     "iopub.status.busy": "2022-05-08T23:07:08.102546Z",
     "iopub.status.idle": "2022-05-08T23:08:06.354741Z",
     "shell.execute_reply": "2022-05-08T23:08:06.353268Z",
     "shell.execute_reply.started": "2022-05-08T23:07:08.102859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load transaction data \n",
    "trans_train = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv',\n",
    "                          dtype={'customer_id': str,'article_id':str})\n",
    "trans_train['quantity']=1\n",
    "trans_train = trans_train[trans_train['t_dat'] >'2020-08-01']\n",
    "\n",
    "# Load article and customers data\n",
    "articles= pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/articles.csv',\n",
    "                      dtype={'article_id': str,'product_code':str})\n",
    "customers = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/customers.csv',\n",
    "                        dtype={'customer_id':str})\n",
    "\n",
    "#Feature transformation\n",
    "master_df = trans_train[['customer_id','article_id','t_dat']].astype(str)\n",
    "master_df['article_id']=master_df['article_id'].apply(zero_f)\n",
    "master_df['quantity'] = trans_train['quantity'].astype(float)\n",
    "masterdf = master_df\n",
    "\n",
    "len(np.unique(trans_train['customer_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T23:08:06.356935Z",
     "iopub.status.busy": "2022-05-08T23:08:06.356272Z",
     "iopub.status.idle": "2022-05-08T23:08:06.380577Z",
     "shell.execute_reply": "2022-05-08T23:08:06.379594Z",
     "shell.execute_reply.started": "2022-05-08T23:08:06.356883Z"
    }
   },
   "outputs": [],
   "source": [
    "masterdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T23:08:06.383024Z",
     "iopub.status.busy": "2022-05-08T23:08:06.382353Z",
     "iopub.status.idle": "2022-05-08T23:08:06.418646Z",
     "shell.execute_reply": "2022-05-08T23:08:06.417715Z",
     "shell.execute_reply.started": "2022-05-08T23:08:06.382980Z"
    }
   },
   "outputs": [],
   "source": [
    "articles.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T23:08:46.186044Z",
     "iopub.status.busy": "2022-05-08T23:08:46.185733Z",
     "iopub.status.idle": "2022-05-08T23:08:46.205761Z",
     "shell.execute_reply": "2022-05-08T23:08:46.204654Z",
     "shell.execute_reply.started": "2022-05-08T23:08:46.186013Z"
    }
   },
   "outputs": [],
   "source": [
    "customers.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T23:08:47.383882Z",
     "iopub.status.busy": "2022-05-08T23:08:47.383206Z",
     "iopub.status.idle": "2022-05-08T23:08:51.546974Z",
     "shell.execute_reply": "2022-05-08T23:08:51.546014Z",
     "shell.execute_reply.started": "2022-05-08T23:08:47.383833Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature Transformation\n",
    "interactions = masterdf\n",
    "interactions['t_dat']=pd.to_datetime(interactions['t_dat'])\n",
    "\n",
    "# Select time frame for model training\n",
    "train = interactions[interactions['t_dat']<='2020-09-15']\n",
    "valid = interactions[(interactions['t_dat'] <='2019-09-17')&(interactions['t_dat'] >'2019-09-15')]\n",
    "test = interactions[interactions['t_dat'] >'2019-09-17']\n",
    "print(len(np.unique(train['customer_id'])))\n",
    "\n",
    "#Batcch processing and loading with tensorflow\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(dict(train[['customer_id',\n",
    "                                                          'article_id']])).shuffle(100_000).batch(256).cache()\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices(dict(valid[['customer_id',\n",
    "                                                          'article_id']])).batch(256).cache()\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(dict(test[['customer_id',\n",
    "                                                        'article_id']])).batch(256).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T23:08:51.584008Z",
     "iopub.status.busy": "2022-05-08T23:08:51.583345Z",
     "iopub.status.idle": "2022-05-08T23:08:52.742260Z",
     "shell.execute_reply": "2022-05-08T23:08:52.741152Z",
     "shell.execute_reply.started": "2022-05-08T23:08:51.583976Z"
    }
   },
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "items_dict = articles[['article_id']].drop_duplicates()\n",
    "customer_dict=customers[['customer_id']].drop_duplicates()\n",
    "\n",
    "items_dict = {name: np.array(value) for name, value in items_dict.items()}\n",
    "customer_dict={name:np.array(value) for name,value in customer_dict.items()}\n",
    "\n",
    "customers=tf.data.Dataset.from_tensor_slices(customer_dict)\n",
    "items = tf.data.Dataset.from_tensor_slices(items_dict)\n",
    "\n",
    "items = items.map(lambda x: x['article_id'])\n",
    "customers=customers.map(lambda x: x['customer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T23:08:52.748453Z",
     "iopub.status.busy": "2022-05-08T23:08:52.747951Z",
     "iopub.status.idle": "2022-05-08T23:08:57.334755Z",
     "shell.execute_reply": "2022-05-08T23:08:57.333769Z",
     "shell.execute_reply.started": "2022-05-08T23:08:52.748406Z"
    }
   },
   "outputs": [],
   "source": [
    "### get unique item and user id's as a lookup table\n",
    "unique_items = np.unique(np.concatenate(list(items.batch(1_000))))\n",
    "unique_user_ids = np.unique(np.concatenate(list(customers.batch(1_000))))\n",
    "\n",
    "# Randomly shuffle data and split between train and test.\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><u>Function my_l1_regularizer and my_positive_weights</u></h3>\n",
    "Input: Trainable weights <br>\n",
    "Output: Updated weights <hr>\n",
    "Description: Converts input weights to positive value and regularized value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T23:08:57.337660Z",
     "iopub.status.busy": "2022-05-08T23:08:57.337416Z",
     "iopub.status.idle": "2022-05-08T23:08:57.343594Z",
     "shell.execute_reply": "2022-05-08T23:08:57.342653Z",
     "shell.execute_reply.started": "2022-05-08T23:08:57.337632Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparamters for model training\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "def my_positive_weights(weights): # return value is just tf.nn.relu(weights)\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><u>Model Training</u></h2>\n",
    "\n",
    "Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T23:08:57.346140Z",
     "iopub.status.busy": "2022-05-08T23:08:57.345409Z",
     "iopub.status.idle": "2022-05-08T23:08:57.358254Z",
     "shell.execute_reply": "2022-05-08T23:08:57.357405Z",
     "shell.execute_reply.started": "2022-05-08T23:08:57.346084Z"
    }
   },
   "outputs": [],
   "source": [
    "class CandidateModel(tfrs.Model):\n",
    "\n",
    "    def __init__(self, user_model, item_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.item_model: tf.keras.Model = item_model\n",
    "        self.user_model: tf.keras.Model = user_model\n",
    "        \n",
    "        ### for retrieval model. we take top-k accuracy as metrics\n",
    "        metrics = tfrs.metrics.FactorizedTopK(candidates=items.batch(256).map(item_model))\n",
    "        \n",
    "        # define the task, which is retrieval                                    )    \n",
    "        task = tfrs.tasks.Retrieval(metrics=metrics)\n",
    "       \n",
    "        self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "        # We pick out the user features and pass them into the user model.\n",
    "        user_embeddings = self.user_model(features[\"customer_id\"])\n",
    "        # And pick out the movie features and pass them into the movie model,\n",
    "        # getting embeddings back.\n",
    "        article_embeddings = self.item_model(features[\"article_id\"])\n",
    "\n",
    "        # The task computes the loss and the metrics.\n",
    "        return self.task(user_embeddings, article_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T23:14:26.433128Z",
     "iopub.status.busy": "2022-05-08T23:14:26.432799Z"
    }
   },
   "outputs": [],
   "source": [
    "### Fitting and evaluating\n",
    "\n",
    "### we choose the dimensionality of the query and candicate representation.\n",
    "embedding_dimension = 64\n",
    "\n",
    "## we pass the model, which is the same model we created in the query and candidate tower, into the model\n",
    "item_model = tf.keras.Sequential([\n",
    "                                tf.keras.layers.StringLookup(\n",
    "                                vocabulary=unique_items, mask_token=None),\n",
    "                                tf.keras.layers.Embedding(len(unique_items) + 1, embedding_dimension)\n",
    "                                ])\n",
    "\n",
    "user_model = tf.keras.Sequential([\n",
    "                                tf.keras.layers.StringLookup(\n",
    "                                vocabulary=unique_user_ids, mask_token=None),\n",
    "                                # We add an additional embedding to account for unknown tokens.\n",
    "                                tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "                                ])\n",
    "\n",
    "model_1 = CandidateModel(user_model, item_model)\n",
    "model_1.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "model_1.fit(train_ds,epochs=5,batch_size=128)\n",
    "model_1.evaluate(test_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><u>Function decoder and run_f</u></h3>\n",
    "Input: feature from submission file <br>\n",
    "Output: decoded and vectorized variable <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(e):\n",
    "    return e.decode('UTF-8')\n",
    "def run_f(item):\n",
    "    _, titles = index(tf.constant([item]),k=12)\n",
    "    t = np.array(titles[0])\n",
    "    vfunc = np.vectorize(decoder)\n",
    "    l = vfunc(t)\n",
    "    l = \" \".join(l)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate submission file for kaggle submission\n",
    "submission_file = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv',\n",
    "                              dtype={'customer_id': str})\n",
    "sub_cust = submission_file[\"customer_id\"]\n",
    "sub_df = pd.DataFrame(columns=['Customer_Id', 'Article_Id'])\n",
    "submission_file[\"prediction\"] = submission_file['customer_id'].progress_apply(run_f)\n",
    "submission_file.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file.to_csv('submission_1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fitting and evaluating\n",
    "\n",
    "### we choose the dimensionality of the query and candicate representation.\n",
    "embedding_dimension = 64\n",
    "\n",
    "## we pass the model, which is the same model we created in the query and candidate tower, into the model\n",
    "item_model = tf.keras.Sequential([\n",
    "                                tf.keras.layers.StringLookup(\n",
    "                                vocabulary=unique_items, mask_token=None),\n",
    "                                tf.keras.layers.Embedding(len(unique_items) + 1, embedding_dimension),\n",
    "                                ])\n",
    "\n",
    "user_model = tf.keras.Sequential([\n",
    "                                tf.keras.layers.StringLookup(\n",
    "                                vocabulary=unique_user_ids, mask_token=None),\n",
    "                                # We add an additional embedding to account for unknown tokens.\n",
    "                                tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension),\n",
    "                                tf.keras.layers.Flatten(),\n",
    "                                tf.keras.layers.Dense((embedding_dimension),activation=\"relu\",\n",
    "                                                      kernel_initializer='he_normal',use_bias=False),\n",
    "                                tf.keras.layers.BatchNormalization(),\n",
    "                                tf.keras.layers.Dropout(0.2),\n",
    "                                tf.keras.layers.Dense((embedding_dimension),activation=\"relu\",\n",
    "                                                      kernel_initializer='he_normal',use_bias=False),\n",
    "                                tf.keras.layers.BatchNormalization(),\n",
    "                                tf.keras.layers.Dropout(0.2),\n",
    "                                tf.keras.layers.Dense((embedding_dimension),\n",
    "                                                      kernel_regularizer=my_l1_regularizer,\n",
    "                                                      kernel_constraint=my_positive_weights)\n",
    "                                ])\n",
    "\n",
    "model_2 = CandidateModel(user_model, item_model)\n",
    "model_2.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "model_2.fit(train_ds,validation_data=valid_ds,epochs=5,batch_size=128)\n",
    "model_2.evaluate(test_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_cust = submission_file[\"customer_id\"]\n",
    "sub_df = pd.DataFrame(columns=['Customer_Id', 'Article_Id'])\n",
    "submission_file[\"prediction\"] = submission_file['customer_id'].progress_apply(run_f)\n",
    "submission_file.head(10)\n",
    "submission_file.to_csv('submission_2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><u>Model Improvement</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fitting and evaluating\n",
    "\n",
    "### we choose the dimensionality of the query and candicate representation.\n",
    "embedding_dimension = 64\n",
    "\n",
    "## we pass the model, which is the same model we created in the query and candidate tower, into the model\n",
    "item_model = tf.keras.Sequential([\n",
    "                                tf.keras.layers.StringLookup(\n",
    "                                vocabulary=unique_items, mask_token=None),\n",
    "                                tf.keras.layers.Embedding(len(unique_items) + 1, embedding_dimension),\n",
    "                                tf.keras.layers.Flatten(),\n",
    "                                tf.keras.layers.Dense((embedding_dimension),activation=\"relu\",\n",
    "                                                      kernel_initializer='he_normal',use_bias=False),\n",
    "                                tf.keras.layers.BatchNormalization(),\n",
    "                                tf.keras.layers.Dense((embedding_dimension),activation=\"relu\",\n",
    "                                                      kernel_initializer='he_normal',use_bias=False),\n",
    "                                tf.keras.layers.BatchNormalization(),\n",
    "                                tf.keras.layers.Dropout(0.2),\n",
    "                                tf.keras.layers.Dense((embedding_dimension),activation=\"softmax\",\n",
    "                                                      kernel_regularizer=my_l1_regularizer,\n",
    "                                                      kernel_constraint=my_positive_weights)\n",
    "                                ])\n",
    "\n",
    "user_model = tf.keras.Sequential([\n",
    "                                tf.keras.layers.StringLookup(\n",
    "                                vocabulary=unique_user_ids, mask_token=None),\n",
    "                                # We add an additional embedding to account for unknown tokens.\n",
    "                                tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension),\n",
    "                                tf.keras.layers.Flatten(),\n",
    "                                tf.keras.layers.Dense((embedding_dimension),activation=\"relu\",\n",
    "                                                      kernel_initializer='he_normal',use_bias=False),\n",
    "                                tf.keras.layers.BatchNormalization(),\n",
    "                                tf.keras.layers.Dense((embedding_dimension),activation=\"relu\",\n",
    "                                                      kernel_initializer='he_normal',use_bias=False),\n",
    "                                tf.keras.layers.BatchNormalization(),\n",
    "                                tf.keras.layers.Dropout(0.2),\n",
    "                                tf.keras.layers.Dense((embedding_dimension),activation=\"relu\",\n",
    "                                                      kernel_initializer='he_normal',use_bias=False),\n",
    "                                tf.keras.layers.BatchNormalization(),\n",
    "                                tf.keras.layers.Dropout(0.2),\n",
    "                                tf.keras.layers.Dense((embedding_dimension),activation=\"relu\",\n",
    "                                                      kernel_initializer='he_normal',use_bias=False),\n",
    "                                tf.keras.layers.BatchNormalization(),\n",
    "                                tf.keras.layers.Dropout(0.2),\n",
    "                                tf.keras.layers.Dense((embedding_dimension),activation=\"relu\",\n",
    "                                                      kernel_initializer='he_normal',use_bias=False),\n",
    "                                tf.keras.layers.BatchNormalization(),\n",
    "                                tf.keras.layers.Dropout(0.2),\n",
    "                                tf.keras.layers.Dense((embedding_dimension),\n",
    "                                                      kernel_regularizer=my_l1_regularizer,\n",
    "                                                      kernel_constraint=my_positive_weights)\n",
    "                                ])\n",
    "\n",
    "model = CandidateModel(user_model, item_model)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "model.fit(train_ds,validation_data=valid_ds,epochs=5,batch_size=128)\n",
    "model.evaluate(test_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabolary bucket of unique items\n",
    "unique_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bucket of unique customer ids\n",
    "unique_user_ids "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><u>Model evaluation</u></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "index.index_from_dataset(items.batch(100).map(lambda items: (items,model.item_model(items))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted recommendations\n",
    "j = '000231cc9af9e58ab4edc66fbd61da921b144ba85bc1c00d0ae2309531e4c210'\n",
    "_, titles = index(tf.constant([j]),k=12)\n",
    "print(f\"Recommendations for user %s: {titles[0]}\" %(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates the process that was used to convert values for each customer into a string of values. These values are added into the csv file. TQDM provides a progress bar that indicates the progress of the transformations indicated with the apply function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_cust = submission_file[\"customer_id\"]\n",
    "sub_df = pd.DataFrame(columns=['Customer_Id', 'Article_Id'])\n",
    "submission_file[\"prediction\"] = submission_file['customer_id'].progress_apply(run_f)\n",
    "submission_file.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file.to_csv('submission_2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
